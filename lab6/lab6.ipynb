{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Regularization and Optimization\n",
    "\n",
    "Nithish Addepalli and Ren Yi, 3-26-2019\n",
    "\n",
    "The goal of this lab is to learn how to apply different regularization and optimization strategies in PyTorch using MNIST data.\n",
    "\n",
    "Here is a list of the techniques we've covered in class\n",
    "- Regularization\n",
    "    - L1/L2 regularization\n",
    "    - Data augmentation\n",
    "    - Dropout\n",
    "    - Batch normalization\n",
    "    - Early stopping\n",
    "- Optimization\n",
    "    - SGD\n",
    "    - SGD (with momentum)\n",
    "    - Nesterov momentum\n",
    "    - AdaGrad\n",
    "    - RMSProp\n",
    "    - Adam\n",
    "    \n",
    "We will show you how some of these methods are used in PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize necessary parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(717)\n",
    "\n",
    "seed = 345\n",
    "batch_size = {'train': 64,\n",
    "              'val': 1000}\n",
    "input_size = 28 * 28\n",
    "output_size = 10\n",
    "n_feature = 3\n",
    "optim_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "trainset = datasets.MNIST('data', train=True, download=True,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "testset = datasets.MNIST('data', train=False, \n",
    "                         transform=transforms.Compose([\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "mnist_datasets = {'train': trainset, \n",
    "            'val': testset}\n",
    "dataset_sizes = {x: len(mnist_datasets[x]) for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(mnist_datasets[x], batch_size=batch_size[x], shuffle=True)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show some images\n",
    "plt.figure(figsize = (12, 8))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = dataloaders['train'].dataset.__getitem__(i)\n",
    "    plt.imshow(image.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, num_epochs=1, verbose = True, print_every = 100):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    loss_dict = {'train': [], 'val': []}\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for batch_idx, data in enumerate(dataloaders[phase]):\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size()[0]\n",
    "                running_corrects += torch.sum(preds == labels).item()\n",
    "                \n",
    "                if verbose and batch_idx % print_every == 0:\n",
    "                    print('Train set | epoch: {:3d} | {:6d}/{:6d} batches | Loss: {:6.4f}'.format(\n",
    "                        epoch, batch_idx * len(inputs), len(dataloaders[phase].dataset), loss.item()))\n",
    "                    loss_dict[phase].append(running_loss/((batch_idx + 1) * len(inputs)))\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            loss_dict[phase].append(epoch_loss)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_dict\n",
    "\n",
    "def populate_result(dictionary, method, train_loss, val_loss):\n",
    "    dictionary[method] = {}\n",
    "    dictionary[method]['train_loss'] = np.array(train_loss)\n",
    "    dictionary[method]['val_loss'] = np.array(val_loss)\n",
    "    \n",
    "def plot_loss(result, loss='train_loss', ylim=None):\n",
    "    plt.plot(result['Baseline'][loss], label='Baseline')\n",
    "    for k in result.keys():\n",
    "        if k != 'Baseline':\n",
    "            plt.plot(result[k][loss], label=k)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_best_loss(result, loss='val_loss'):\n",
    "    labels = ['Baseline']\n",
    "    acc = [np.max(result['Baseline'][loss])]\n",
    "    for k in result.keys():\n",
    "        if k != 'Baseline':\n",
    "            labels.append(k)\n",
    "            acc.append(np.max(result[k][loss]))\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    plt.barh(x, acc)\n",
    "    plt.yticks(x, labels)\n",
    "    plt.xlabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline method\n",
    "\n",
    "We will later show how different optimization and regularization techniques can improve baseline model performance. But first,\n",
    "1. What's our baseline model architecture?\n",
    "2. What's the optimization method used to train the baseline model?\n",
    "3. How does this optimization method update its parameters.\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\eta \\nabla J(\\theta_{t})$$\n",
    "where $\\eta$ denotes the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self, n_feature, output_size):\n",
    "        super(Baseline, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, n_feature, kernel_size=5),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(n_feature, n_feature, kernel_size=5),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2))\n",
    "        self.fc = nn.Sequential(nn.Linear(n_feature*4*4, 50),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(50, 10))\n",
    "\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(-1, self.n_feature*4*4)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(n_feature, output_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "model, loss_dict = train_model(model, optimizer)\n",
    "populate_result(optim_results, 'Baseline', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fancier Optimization\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with momentum\n",
    "1. How does SGD with momentum update its parameters?\n",
    "$$v_{t+1} = \\rho v_{t} + \\nabla J(\\theta_{t})$$\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\eta v_{t+1}$$\n",
    "where $v$ and $\\rho$ denote velocity and momentum, respectively.\n",
    "2. Check out the documentation for SGD in PyTorch and complete the code below (Set momentum=0.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(n_feature, output_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01, momentum=0.5)\n",
    "model, loss_dict = train_model(model, optimizer)\n",
    "populate_result(optim_results, 'SGD_momentum', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov momentum\n",
    "\n",
    "Make a minor change in the above code to apply Nesterov momentum (Set momentum=0.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(n_feature, output_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01, momentum=0.5, nesterov=True)\n",
    "model, loss_dict = train_model(model, optimizer)\n",
    "populate_result(optim_results, 'Nesterov_momentum', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "1. What's the motivation of AdaGrad?\n",
    "2. Check out the documentation of AdaGrad in PyTorch and complete the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(n_feature, output_size).to(device)\n",
    "optimizer = optim.Adagrad(model.parameters(),lr=0.01)\n",
    "model, loss_dict = train_model(model, optimizer)\n",
    "populate_result(optim_results, 'Adagrad', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(n_feature, output_size).to(device)\n",
    "optimizer = optim.RMSprop(model.parameters(),lr=0.01, alpha=0.9)\n",
    "model, loss_dict = train_model(model, optimizer)\n",
    "populate_result(optim_results, 'RMSprop', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(n_feature, output_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01, betas=(0.9, 0.99))\n",
    "model, loss_dict = train_model(model, optimizer)\n",
    "populate_result(optim_results, 'Adam', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization methods training results\n",
    "plot_loss(optim_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization methods validation results\n",
    "plot_best_loss(optim_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to see effects of regularization on validation set, we need to make some slight modification\n",
    "\n",
    "## Smaller training set\n",
    "mnist_datasets['train'].data = mnist_datasets['train'].data[:600]\n",
    "dataset_sizes = {x: len(mnist_datasets[x]) for x in ['train', 'val']}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(mnist_datasets['train'], \n",
    "                                                   batch_size=batch_size['train'], shuffle=True)\n",
    "\n",
    "## Longer training epochs\n",
    "train_epochs = 25\n",
    "verbose = False\n",
    "reg_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(n_feature, output_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "model, loss_dict = train_model(model, optimizer, num_epochs=train_epochs, verbose=verbose)\n",
    "populate_result(optim_results, 'Baseline', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1/L2 regularization\n",
    "\n",
    "L2 regularization is included in most optimizers in PyTorch and can be controlled with the __weight_decay__ parameter.\n",
    "For L1 regularization, check out this post: https://discuss.pytorch.org/t/simple-l2-regularization/139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Dropout Layer\n",
    "\n",
    "1. How does Dropout introduce regularization effect?\n",
    "2. Check out documentations for __nn.Dropout()__. Modify the Baseline model and add dropout layer to the fully connected layers.\n",
    "3. Optionally, you may also check out documentations for __nn.Dropout2d()__ to learn how to add Dropout layer to convolution layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutNet(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size, dropout_rate=0.5):\n",
    "        super(DropoutNet, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, n_feature, kernel_size=5),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(n_feature, n_feature, kernel_size=5),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2))\n",
    "        self.fc = nn.Sequential(nn.Linear(n_feature*4*4, 50),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(dropout_rate),\n",
    "                                nn.Linear(50, 10))\n",
    "\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(-1, self.n_feature*4*4)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = DropoutNet(input_size, n_feature, output_size, dropout_rate=0.3).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "model, loss_dict = train_model(model, optimizer, num_epochs=train_epochs, verbose=verbose)\n",
    "populate_result(optim_results, 'Dropout', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Batch Normalization\n",
    "1. What's the advantage of using batch normalization?\n",
    "    1. Allow each layer to train relatively more independently\n",
    "    2. Improve gradient flow through the network\n",
    "    3. Allows higher learning rate\n",
    "    4. Reduces strong dependence on initialization\n",
    "2. Batch normalization also act as a form of regularization, why?\n",
    "3. Implement batch normalization in __BatchnormNet__. Think about where you may want to insert the batch normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchnormNet(nn.Module):\n",
    "    def __init__(self, n_feature, output_size):\n",
    "        super(BatchnormNet, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, n_feature, kernel_size=5),\n",
    "                                   nn.BatchNorm2d(n_feature),\n",
    "                                   nn.MaxPool2d(2),\n",
    "                                   nn.ReLU())\n",
    "        \n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(n_feature, n_feature, kernel_size=5),\n",
    "                                   nn.BatchNorm2d(n_feature),\n",
    "                                   nn.MaxPool2d(2),\n",
    "                                   nn.ReLU())\n",
    "        self.fc = nn.Sequential(nn.Linear(n_feature*4*4, 50),\n",
    "                                nn.BatchNorm1d(50),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(50, 10))\n",
    "\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(-1, self.n_feature*4*4)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = BatchnormNet(n_feature, output_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "model, loss_dict = train_model(model, optimizer, num_epochs=train_epochs, verbose=verbose)\n",
    "populate_result(optim_results, 'Batchnorm', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(reg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss using Dropout and Batchnorm start to catch up with the loss of baseline model after baseline model overfits training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(reg_results, loss = 'val_loss', ylim=(0.1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: parameter tunning\n",
    "\n",
    "We've introduced multiple regularization and optimization techniques to improve your model. How can you combine these techniques and perform grid search to find out a set of parameters that maximize your model performance on validation set? Are there other model architectures you'd like to try?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
